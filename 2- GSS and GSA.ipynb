{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "001cf284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T08:52:59.457744Z",
     "start_time": "2023-04-19T08:52:49.717965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\diametrix\\appdata\\roaming\\python\\python39\\site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\diametrix\\appdata\\roaming\\python\\python39\\site-packages (from python-Levenshtein) (63.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jaro-winkler in c:\\users\\diametrix\\anaconda3\\lib\\site-packages (2.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting strsim\n",
      "  Downloading strsim-0.0.3-py3-none-any.whl (42 kB)\n",
      "     -------------------------------------- 42.4/42.4 kB 411.8 kB/s eta 0:00:00\n",
      "Installing collected packages: strsim\n",
      "Successfully installed strsim-0.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\diametrix\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install jaro-winkler\n",
    "!pip install strsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4fd40c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:51:55.005419Z",
     "start_time": "2023-06-28T12:51:54.080849Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "from Levenshtein import distance as lev\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import jaro\n",
    "normalized_levenshtein = NormalizedLevenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "64ed87f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:39.474821Z",
     "start_time": "2023-06-28T12:51:55.081189Z"
    }
   },
   "outputs": [],
   "source": [
    "gsa=pd.read_csv(r\"Y:\\REFERENTIEL DATA\\RP 2022 FRANCE\\GEODATA - Reseaux de distibution\\GSA_GSS 2020\"+r'\\GSA_2022_clean.csv')\n",
    "gsa = gpd.GeoDataFrame(\n",
    "    gsa, geometry=gpd.points_from_xy(gsa.X__WGS84_, gsa.Y__WGS84_)).set_crs(\"EPSG:4326\").to_crs(\"2154\")\n",
    "data=pd.read_excel(r\"Y:\\REFERENTIEL DATA\\RP 2022 FRANCE\\GEODATA - IRIS\\Geodata_IRIS_essentiels.xlsx\")\n",
    "list_departement=pd.read_excel(r\"Y:\\RECHERCHE ET DEV\\10_Modèle d'attractivite des rues\\\\Liste-Excel-des-departements-francais.xlsx\")\n",
    "contour_iris=gpd.read_file(r'Y:\\REFERENTIEL DATA\\RP 2022 FRANCE\\CARTOGRAPHIE\\Cartographie_France_IRIS_2019\\CONTOURS-IRIS_2-1_SHP_LAMB93_FXX-2019\\CONTOURS-IRIS.shp').set_crs(\"EPSG:2154\")\n",
    "gss=pd.read_pickle(r\"Y:\\REFERENTIEL DATA\\RP 2022 FRANCE\\GEODATA - Reseaux de distibution\\GSA_GSS 2020\"+r'\\GSA_GSS_actif_iris_2020_geo_gss.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "17238930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:39.649356Z",
     "start_time": "2023-06-28T12:53:39.475819Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace nan values of coordinates\n",
    "nan_values_coordinates=gss[gss[['X__WGS84_','Y__WGS84_']].isna().any(axis=1)]\n",
    "gss.loc[gss[['X__WGS84_','Y__WGS84_']].isna().any(axis=1),['X__WGS84_','Y__WGS84_']]=nan_values_coordinates.apply(lambda x :get_coordinates(x),axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ae21b3f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:39.790978Z",
     "start_time": "2023-06-28T12:53:39.651350Z"
    }
   },
   "outputs": [],
   "source": [
    "gss=gss[~gss[['X__WGS84_','Y__WGS84_']].isna().any(axis=1)]\n",
    "gss = gpd.GeoDataFrame(\n",
    "    gss, geometry=gpd.points_from_xy(gss.X__WGS84_, gss.Y__WGS84_)).set_crs(\"EPSG:4326\").to_crs(\"EPSG:2154\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2fb2b79e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:40.200885Z",
     "start_time": "2023-06-28T12:53:39.791976Z"
    }
   },
   "outputs": [],
   "source": [
    "logs=pickle.load(open(r\"Y:\\RECHERCHE ET DEV\\10_Modèle d'attractivite des rues\\04_données clients (source)\\\\logs.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0f434fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:40.247760Z",
     "start_time": "2023-06-28T12:53:40.201882Z"
    }
   },
   "outputs": [],
   "source": [
    "def distance(x,gsa_name):\n",
    "    normalized_levenshtein = NormalizedLevenshtein()\n",
    "    min_distance=1\n",
    "    for el in x.split(' '):\n",
    "        for el_ in gsa_name.split(\" \"):\n",
    "        \n",
    "             if min_distance >normalized_levenshtein.distance(el.lower(),el_.lower()):\n",
    "                min_distance=normalized_levenshtein.distance(el.lower(),el_.lower())\n",
    "    return min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b6966202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:40.262721Z",
     "start_time": "2023-06-28T12:53:40.248758Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_coordinates(x):\n",
    "    try:\n",
    "        return ox.geocode(str(x[\"N__Nom_de_voie\"])+\" \"+str(x['CP']))\n",
    "    except:\n",
    "        return (None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed44a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T07:13:05.759303Z",
     "start_time": "2023-06-26T07:13:05.682576Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1dce3b",
   "metadata": {},
   "source": [
    "# Deduplication et correspondance GSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "110515b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:40.294635Z",
     "start_time": "2023-06-28T12:53:40.263718Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dedup_gsa(gsa_dep,path_to_save,shops):\n",
    "    correspondance=pd.DataFrame(columns =[\"shop\",\"gsa\",\"distance\",\"name_shop\",\"name_gsa\"])\n",
    "    for i in range(len(gsa_dep)):\n",
    "        #take all  the shops in circle of 100m  diamater\n",
    "        #check the name of GSA and all other shops in this cirlce\n",
    "        d=shops.distance(gsa_dep['geometry'].iloc[i]).sort_values()\n",
    "\n",
    "        d=d[d<=100]\n",
    "\n",
    "\n",
    "\n",
    "        #if there is some shops  nearby\n",
    "        if len(d)>0:\n",
    "\n",
    "            shops_nearby=shops.loc[d.index,:]\n",
    "\n",
    "            shops_nearby_names=shops_nearby['name']\n",
    "            gsa_name=gsa_dep.iloc[i]['Nom_Enseigne']\n",
    "\n",
    "\n",
    "            #fill null values\n",
    "            shops_nearby_names=shops_nearby_names.fillna(\"\")\n",
    "\n",
    "            #if one of the nearby shops has the same name as GSA/GSS then we associate them to each other\n",
    "            a=np.array([i.strip().lower() for i in shops_nearby_names])\n",
    "            a=np.array([i.replace('city',\"\").replace('market',\"\").replace('g20',\"g 20\") for  i in a])\n",
    "            if gsa_name.strip().lower() in a:\n",
    "                shop_id=shops_nearby_names[a==gsa_name.strip().lower()].index[0]\n",
    "                shop_name=shops_nearby.loc[shop_id]['name']\n",
    "                #name_dhopshops_nearby_names[a==(gsa_name.strip().lower())].to_numpy()[0])\n",
    "                correspondance.loc[len(correspondance.index)]=[shop_id,i,0,shop_name,gsa_name]\n",
    "                continue\n",
    "\n",
    "            lev_distances=shops_nearby_names.apply(lambda x :distance(x,gsa_name))\n",
    "            lev_distances=lev_distances[lev_distances<=0.25]\n",
    "\n",
    "            #if there is no shop with the same name as GSA , we associate the GSA to the colsest shop \n",
    "            if len(lev_distances)>0:\n",
    "\n",
    "                index_sim=lev_distances.index[lev_distances.argmin()]\n",
    "                correspondance.loc[len(correspondance.index)]=[index_sim,i,d[index_sim],shops.loc[index_sim,\"name\"],gsa_name]\n",
    "            elif str(shops.loc[d.index[d.argmin()],\"name\"]).strip().lower()==\"NaN\":\n",
    "                closest_index=d.index[d.argmin()]\n",
    "                correspondance.loc[len(correspondance.index)]=[closest_index,i,d[closest_index],shops.loc[closest_index,\"name\"],gsa_name]\n",
    "    correspondance['name_shop']=correspondance['name_shop'].fillna(\"\")\n",
    "    \n",
    "    #Remove duplicates\n",
    "    #This happens when we associate the same shop to two or more different GSA\n",
    "\n",
    "    counts=correspondance['shop'].value_counts()\n",
    "    duplicated=[i   for i,x in correspondance.iterrows()  if x[\"shop\"] in counts.index[counts>1]]\n",
    "    duplicated_shops=correspondance[correspondance['shop'].isin(counts.index[counts>1])]\n",
    "    all_index=[i for i in range(len(correspondance))]\n",
    "    not_duplicated_index =[i for i in all_index if i not in duplicated]\n",
    "    for index in list(set(duplicated_shops['shop'])):\n",
    "\n",
    "        a=duplicated_shops[duplicated_shops[\"shop\"]==index]\n",
    "\n",
    "        a[\"edit_distance\"]=a.apply(lambda x :normalized_levenshtein.distance(x['name_shop'].lower(),x['name_gsa'].lower()),axis = 1)\n",
    "        a=a.sort_values(\"edit_distance\")\n",
    "\n",
    "        #the name of GSA and the shop is so close \n",
    "        #example  name_shop ==Bio C' Bon et name_gsa=INTERMARCHE EXPRESS or  BIO C BON  \n",
    "        #we consider BIO C BON\n",
    "\n",
    "        if len(a[a[\"edit_distance\"]<=0.2])>0:\n",
    "\n",
    "            not_duplicated_index.append(a[a[\"edit_distance\"]<=0.2].index[0])\n",
    "        \n",
    "        else:\n",
    "            #in this case, we consider the min distance\n",
    "      \n",
    "            min_index=a.sort_values(\"distance\").index[0]\n",
    "\n",
    "            not_duplicated_index.append(min_index)\n",
    "    correspondance=correspondance.iloc[not_duplicated_index]\n",
    "    \n",
    "    pickle.dump(correspondance,open(path_to_save+r\"\\\\association\\\\GSA_association.pkl\",\"wb\"))\n",
    "    return correspondance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618504e",
   "metadata": {},
   "source": [
    "# Deduplication et correspondance GSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "715f728f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T12:53:40.310593Z",
     "start_time": "2023-06-28T12:53:40.296630Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dedup_gss(gss_dep,path_to_save,shops):\n",
    "\n",
    "    correspondance_gss=pd.DataFrame(columns =[\"shop\",\"gss\",\"distance\",\"name_shop\",\"name_gss\"])\n",
    "    for i in range(len(gss_dep)):\n",
    "        #take all  the shops in circle of 100m  diamater\n",
    "        #check the name of GSA and all other shops in this cirlce\n",
    "        d=shops.distance(gss_dep['geometry'].iloc[i]).sort_values()\n",
    "        \n",
    "        d=d[d<=100]\n",
    "\n",
    "        #if there is some shops in the proximity\n",
    "        if len(d)>0:\n",
    "            shops_nearby=shops.loc[d.index,:]\n",
    "\n",
    "            shops_nearby_names=shops_nearby['name']\n",
    "            gss_name=str(gss_dep.iloc[i]['Nom_Enseigne'])\n",
    "\n",
    "            #fill null values\n",
    "            shops_nearby_names=shops_nearby_names.fillna(\"\")\n",
    "\n",
    "            a=np.array([i.strip().lower() for i in shops_nearby_names])\n",
    "            a=np.array([i.replace('city',\"\").replace('market',\"\").replace('g 20',\"g20\") for  i in a])\n",
    "            if gss_name.strip().lower() in a:\n",
    "                shop_id=shops_nearby_names[a==gss_name.strip().lower()].index[0]\n",
    "                shop_name=shops_nearby.loc[shop_id]['name']\n",
    "                #name_dhopshops_nearby_names[a==(gsa_name.strip().lower())].to_numpy()[0])\n",
    "                correspondance_gss.loc[len(correspondance_gss.index)]=[shop_id,i,0,shop_name,gss_name]\n",
    "                continue\n",
    "\n",
    "            lev_distances=shops_nearby_names.apply(lambda x :distance(x,gss_name))\n",
    "            lev_distances=lev_distances[lev_distances<=0.1]\n",
    "\n",
    "\n",
    "            #if there is no shop with the same name as GSA , we associate the GSA to the colsest shop \n",
    "            if len(lev_distances)>0:\n",
    "\n",
    "                index_sim=lev_distances.index[lev_distances.argmin()]\n",
    "                correspondance_gss.loc[len(correspondance_gss.index)]=[index_sim,i,d[index_sim],shops.loc[index_sim,\"name\"],gss_name]\n",
    "            elif str(shops.loc[d.index[d.argmin()],\"name\"]).strip().lower()==\"nan\":\n",
    "                closest_index=d.index[d.argmin()]\n",
    "                correspondance_gss.loc[len(correspondance_gss.index)]=[closest_index,i,d[closest_index],shops.loc[closest_index,\"name\"],gss_name]\n",
    "    correspondance_gss['name_shop']=correspondance_gss['name_shop'].fillna(\"\")\n",
    "    counts=correspondance_gss['shop'].value_counts()\n",
    "    duplicated=[i   for i,x in correspondance_gss.iterrows()  if x[\"shop\"] in counts.index[counts>1]]\n",
    "    duplicated_shops=correspondance_gss[correspondance_gss['shop'].isin(counts.index[counts>1])]\n",
    "    all_index=[i for i in range(len(correspondance_gss))]\n",
    "    not_duplicated_index =[i for i in all_index if i not in duplicated]\n",
    "    for index in list(set(duplicated_shops['shop'])):\n",
    "    \n",
    "        a=duplicated_shops[duplicated_shops[\"shop\"]==index]\n",
    "\n",
    "        a[\"edit_distance\"]=a.apply(lambda x :normalized_levenshtein.distance(x['name_shop'].lower(),x['name_gss'].lower()),axis = 1)\n",
    "        a=a.sort_values(\"edit_distance\")\n",
    "    \n",
    "        #the name of GSA and the shop is so close \n",
    "        #example  name_shop ==Bio C' Bon et name_gsa=INTERMARCHE EXPRESS or  BIO C BON  \n",
    "        #we consider BIO C BON\n",
    "\n",
    "        if len(a[a[\"edit_distance\"]<=0.2])>0:\n",
    "\n",
    "            not_duplicated_index.append(a[a[\"edit_distance\"]<=0.2].index[0])\n",
    "        \n",
    "        else:\n",
    "            #in this case, we consider the min distance\n",
    "            min_index=a.sort_values(\"distance\").index[0]\n",
    "\n",
    "            not_duplicated_index.append(min_index)\n",
    "    correspondance_gss=correspondance_gss.iloc[not_duplicated_index]\n",
    "    pickle.dump(correspondance_gss,open(path_to_save+r\"\\\\association\\\\GSS_association.pkl\",\"wb\"))\n",
    "    return correspondance_gss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "40367670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T13:01:58.751637Z",
     "start_time": "2023-06-28T13:01:48.928616Z"
    }
   },
   "outputs": [],
   "source": [
    "for i ,row in list_departement.iterrows():\n",
    "    try:\n",
    "\n",
    "        place_name=row[\"Département\"].strip().lower()\n",
    "        path=r\"Y:\\RECHERCHE ET DEV\\10_Modèle d'attractivite des rues\\04_données clients (source)\\osm\\\\\"+row['Région'].lower()+r\"\\\\\"+row[\"Département\"].lower()\n",
    "        if place_name==\"rhône\":\n",
    "            try:\n",
    "                dep=int(row['N°'])\n",
    "                gsa_dep=gsa[gsa['CP'].isin([i for i in range(dep*1000,(dep+1)*1000) ])]\n",
    "                gss_dep=gss[gss['CP'].isin([i for i in range(dep*1000,(dep+1)*1000) ])]\n",
    "            except:\n",
    "                gsa_dep=gsa.copy()\n",
    "                gss_dep=gss.copy()\n",
    "\n",
    "            if len(gsa_dep)>0:\n",
    "                shops=pickle.load(open(r\"Y:\\RECHERCHE ET DEV\\10_Modèle d'attractivite des rues\\04_données clients (source)\\osm\\\\\"+row['Région'].lower()+r\"\\\\\"+row[\"Département\"].lower()+\"\\\\raw data\\\\shops.pkl\",'rb')).to_crs('EPSG:2154')\n",
    "                correspondance=dedup_gsa(gsa_dep,path,shops)\n",
    "\n",
    "                if place_name=='paris' or place_name=='haute-corse':\n",
    "                    print(place_name)\n",
    "                    print(len(correspondance))\n",
    "                logs[place_name].update({\"correspondance_gsa\":len(correspondance)})\n",
    "            if  len(gsa_dep)>0:\n",
    "                shops=pickle.load(open(r\"Y:\\RECHERCHE ET DEV\\10_Modèle d'attractivite des rues\\04_données clients (source)\\osm\\\\\"+row['Région'].lower()+r\"\\\\\"+row[\"Département\"].lower()+\"\\\\raw data\\\\shops.pkl\",'rb')).to_crs('EPSG:2154')\n",
    "                correspondance_gss=dedup_gss(gss_dep,path,shops)\n",
    "                logs[place_name].update({\"correspondance_gss\":len(correspondance_gss)})\n",
    "\n",
    "    except:\n",
    "        print(row[\"Département\"])\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "pickle.dump(logs,open(r\"Y:\\RECHERCHE ET DEV\\10_Modèle d'attractivite des rues\\04_données clients (source)\\\\logs.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc3a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
